{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 4: Chaining Prompts for Agentic Reasoning\n",
        "\n",
        "## Automated Claim Triage: From First-Notice to the Right Queue\n",
        "\n",
        "In this hands-on exercise, you will build a prompt chain that extracts key fields from free-form auto-claim reports, assesses damage severity, and routes each claim to one of several queues ‚Äî `glass`, `fast_track`, `material_damage`, or `total_loss` ‚Äî with code-based gate checks at every step.\n",
        "\n",
        "## Outline:\n",
        "\n",
        "- Setup\n",
        "- Sample FNOL (First Notice of Loss) Texts\n",
        "- Stage I: Information Extraction\n",
        "- Stage II: Severity Assessment\n",
        "- Stage III: Queue Routing\n",
        "- Review Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary libraries and define helper functions, including a mock LLM client, code execution environment, and test runner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "# No changes needed in this cell\n",
        "from openai import OpenAI  # For accessing the OpenAI API\n",
        "from enum import Enum\n",
        "import json\n",
        "from pydantic import BaseModel, Field  # For structured data validation\n",
        "from typing import List, Literal, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up LLM credentials\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openai.vocareum.com/v1\",\n",
        "    # Uncomment one of the following\n",
        "    api_key=\"voc-18578903711607364724451696893a3acedc2.01924345\",  # <--- TODO: Fill in your Vocareum API key here\n",
        "    # api_key=os.getenv(\n",
        "    #     \"OPENAI_API_KEY\"\n",
        "    # ),  # <-- Alternately, set as an environment variable\n",
        ")\n",
        "\n",
        "# If using OpenAI's API endpoint\n",
        "# client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "class OpenAIModels(str, Enum):\n",
        "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
        "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
        "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
        "\n",
        "\n",
        "MODEL = OpenAIModels.GPT_41_NANO\n",
        "\n",
        "\n",
        "def get_completion(messages=None, system_prompt=None, user_prompt=None, model=MODEL):\n",
        "    \"\"\"\n",
        "    Function to get a completion from the OpenAI API.\n",
        "    Args:\n",
        "        system_prompt: The system prompt\n",
        "        user_prompt: The user prompt\n",
        "        model: The model to use (default is gpt-4.1-mini)\n",
        "    Returns:\n",
        "        The completion text\n",
        "    \"\"\"\n",
        "\n",
        "    messages = list(messages)\n",
        "    if system_prompt:\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
        "    if user_prompt:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample FNOL (First Notice of Loss) Texts\n",
        "Let's define a few sample First Notice of Loss (FNOL) texts to process through our chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample FNOL texts\n",
        "# TODO: [Optional] Add more sample FNOL texts to test various scenarios\n",
        "\n",
        "sample_fnols = [\n",
        "    \"\"\"\n",
        "    Claim ID: C001\n",
        "    Customer: John Smith\n",
        "    Vehicle: 2018 Toyota Camry\n",
        "    Incident: While driving on the highway, a rock hit my windshield and caused a small chip\n",
        "    about the size of a quarter. No other damage was observed.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Claim ID: C002\n",
        "    Customer: Sarah Johnson\n",
        "    Vehicle: 2020 Honda Civic\n",
        "    Incident: I was parked at the grocery store and returned to find someone had hit my car and\n",
        "    dented the rear bumper and taillight. The taillight is broken and the bumper has a large dent.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Claim ID: C003\n",
        "    Customer: Michael Rodriguez\n",
        "    Vehicle: 2022 Ford F-150\n",
        "    Incident: I was involved in a serious collision at an intersection. The front of my truck is\n",
        "    severely damaged, including the hood, bumper, radiator, and engine compartment. The airbags\n",
        "    deployed and the vehicle is not drivable.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Claim ID: C004\n",
        "    Customer: Emma Williams\n",
        "    Vehicle: 2019 Subaru Outback\n",
        "    Incident: My car was damaged in a hailstorm. There are multiple dents on the hood, roof, and\n",
        "    trunk. The side mirrors were also damaged and one window has a small crack.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Claim ID: C005\n",
        "    Customer: David Brown\n",
        "    Vehicle: 2021 Tesla Model 3\n",
        "    Incident: Someone keyed my car in the parking lot. There are deep scratches along both doors\n",
        "    on the driver's side.\n",
        "    \"\"\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage I: Information Extraction\n",
        "In this stage, we'll create a prompt that extracts structured information from free-form FNOL text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a system prompt for information extraction according to the provided ClaimInformation class\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "class ClaimInformation(BaseModel):\n",
        "    claim_id: str = Field(..., min_length=2, max_length=10)\n",
        "    name: str = Field(..., min_length=2, max_length=100)\n",
        "    vehicle: str = Field(..., min_length=2, max_length=100)\n",
        "    loss_desc: str = Field(..., min_length=10, max_length=500)\n",
        "    damage_area: List[\n",
        "        Literal[\n",
        "            \"windshield\",\n",
        "            \"front\",\n",
        "            \"rear\",\n",
        "            \"side\",\n",
        "            \"roof\",\n",
        "            \"hood\",\n",
        "            \"door\",\n",
        "            \"bumper\",\n",
        "            \"fender\",\n",
        "            \"quarter panel\",\n",
        "            \"trunk\",\n",
        "            \"glass\",\n",
        "        ]\n",
        "    ] = Field(..., min_length=1)\n",
        "\n",
        "\n",
        "info_extraction_system_prompt = \"\"\"\n",
        "You are an auto insurance claim processing assistant. Your task is to extract key information from First Notice of Loss (FNOL) reports.\n",
        "\n",
        "Format your response as a valid JSON object with the following keys:\n",
        "- claim_id (str): The claim ID\n",
        "- name (str): The customer's full name\n",
        "- vehicle (str): The vehicle make and model\n",
        "- loss_desc (str): A brief description of the loss incident\n",
        "- damage_area (List[str]): A list of damaged areas of the vehicle. Possible values include:\n",
        "  - windshield\n",
        "  - front\n",
        "  - rear\n",
        "  - side\n",
        "  - roof\n",
        "  - hood\n",
        "  - door\n",
        "  - bumper\n",
        "  - fender\n",
        "  - quarter panel\n",
        "  - trunk\n",
        "  - glass\n",
        "\n",
        "For damage_area, only use items from the list above.\n",
        "\n",
        "Only respond with the JSON object, nothing else.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a gate check function and claim extraction function\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "def gate1_validate_claim_info(claim_info_json: str) -> ClaimInformation:\n",
        "    \"\"\"\n",
        "    Gate 1: Validates claim information extracted from FNOL text.\n",
        "    Returns validated ClaimInformation object or raises validation error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON string\n",
        "        claim_info_dict = json.loads(claim_info_json)\n",
        "        # Validate with Pydantic model\n",
        "        validated_info = ClaimInformation(**claim_info_dict)\n",
        "        return validated_info\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Gate 1 validation failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def extract_claim_info(fnol_text):\n",
        "    \"\"\"\n",
        "    Stage 1: Extract structured information from FNOL text\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": info_extraction_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": fnol_text},\n",
        "    ]\n",
        "\n",
        "    response = get_completion(messages=messages)\n",
        "\n",
        "    # Gate check: validate the extracted information\n",
        "    try:\n",
        "        validated_info = gate1_validate_claim_info(response)\n",
        "        return validated_info\n",
        "    except ValueError as e:\n",
        "        print(f\"Gate 1 failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ClaimInformation(claim_id='C001', name='John Smith', vehicle='2018 Toyota Camry', loss_desc='While driving on the highway, a rock hit my windshield and caused a small chip about the size of a quarter. No other damage was observed.', damage_area=['windshield']),\n",
              " ClaimInformation(claim_id='C002', name='Sarah Johnson', vehicle='2020 Honda Civic', loss_desc='Someone hit the parked car, damaging the rear bumper and taillight', damage_area=['bumper', 'rear']),\n",
              " ClaimInformation(claim_id='C003', name='Michael Rodriguez', vehicle='2022 Ford F-150', loss_desc='Serious collision at intersection with severe damage to the front, including hood, bumper, radiator, and engine compartment. Airbags deployed, vehicle not drivable.', damage_area=['front', 'hood', 'bumper']),\n",
              " ClaimInformation(claim_id='C004', name='Emma Williams', vehicle='2019 Subaru Outback', loss_desc='Damaged in a hailstorm with dents on hood, roof, trunk, damaged side mirrors, and a cracked window', damage_area=['hood', 'roof', 'trunk', 'side', 'glass']),\n",
              " ClaimInformation(claim_id='C005', name='David Brown', vehicle='2021 Tesla Model 3', loss_desc=\"Someone keyed my car in the parking lot. There are deep scratches along both doors on the driver's side.\", damage_area=['door'])]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run the claim extraction function on the sample FNOLs\n",
        "# No updates needed in this cell\n",
        "\n",
        "extracted_claim_info_items = [\n",
        "    extract_claim_info(fnol_text) for fnol_text in sample_fnols\n",
        "]\n",
        "extracted_claim_info_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage II: Severity Assessment\n",
        "In this stage, we'll assess the severity of the damage based on the extracted information.\n",
        "\n",
        "Note, our carrier applies the following heuristics:\n",
        "- Minor damage: Small dents, scratches, glass chips (cost range: $100-$1,000)\n",
        "- Moderate damage: Single panel damage, bumper replacement, door damage (cost range: $1,000-$5,000)\n",
        "- Major damage: Structural damage, multiple panel replacement, engine/drivetrain issues, total loss candidates (cost range: $5,000-$50,000)\n",
        "\n",
        "In this example we will let the LLM estimate the cost, though in production we would want a more accurate estimate, e.g. querying a database of repair costs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a system prompt for severity assessment according to the provided SeverityAssessment class\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "class SeverityAssessment(BaseModel):\n",
        "    severity: Literal[\"Minor\", \"Moderate\", \"Major\"]\n",
        "    est_cost: float = Field(..., gt=0)\n",
        "\n",
        "\n",
        "severity_assessment_system_prompt = \"\"\"\n",
        "You are an auto insurance damage assessor. Your task is to evaluate the severity of vehicle damage and estimate repair costs.\n",
        "\n",
        "Apply these carrier heuristics:\n",
        "- Minor damage: Cosmetic issues like scratches, small dents, or minor glass chips. Estimated cost: <$1,000.\n",
        "- Moderate damage: Functional damage that affects vehicle operation but is repairable, such as broken lights\n",
        "- Major damage: Severe damage affecting vehicle structure or safety, such as frame damage or engine issues. Estimated cost: >$5,000.\n",
        "\n",
        "Based on the provided claim information, assess the severity of the damage and estimate the repair cost.\n",
        "1. Severity levels:\n",
        "   - Minor\n",
        "   - Moderate\n",
        "   - Major\n",
        "2. Estimated repair cost in USD.\n",
        "\n",
        "Format your response as a valid JSON object with the following keys:\n",
        "- severity (str): One of \"Minor\", \"Moderate\", or \"Major\"\n",
        "- est_cost (float): Estimated repair cost in USD (e.g., 2500.00)\n",
        "\n",
        "Only respond with the JSON object, nothing else.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a gate check function and assess_severity function\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "def gate2_cost_range_ok(severity_json: str) -> SeverityAssessment:\n",
        "    \"\"\"\n",
        "    Gate 2: Validates that the estimated cost is within reasonable range for the severity.\n",
        "    Returns validated SeverityAssessment object or raises validation error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON string\n",
        "        severity_dict = json.loads(severity_json)\n",
        "        # Validate with Pydantic model\n",
        "        validated_severity = SeverityAssessment(**severity_dict)\n",
        "\n",
        "        # Check cost range based on severity\n",
        "        if (\n",
        "            validated_severity.severity == \"Minor\"\n",
        "            and (\n",
        "                validated_severity.est_cost > 1000\n",
        "            )\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Minor damage should cost between $100-$1000, got ${validated_severity.est_cost}\"\n",
        "            )\n",
        "        elif (\n",
        "            validated_severity.severity == \"Moderate\"\n",
        "            and (\n",
        "                validated_severity.est_cost < 1000 or validated_severity.est_cost > 5000\n",
        "            )\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Moderate damage should cost between $1000-$5000, got ${validated_severity.est_cost}\"\n",
        "            )\n",
        "        elif (\n",
        "            validated_severity.severity == \"Major\"\n",
        "            and (\n",
        "                validated_severity.est_cost < 5000 or validated_severity.est_cost > 50000\n",
        "            )\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Major damage should cost between $5000-$50000, got ${validated_severity.est_cost}\"\n",
        "            )\n",
        "\n",
        "\n",
        "        return validated_severity\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Gate 2 validation failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def assess_severity(claim_info: ClaimInformation) -> Optional[SeverityAssessment]:\n",
        "    \"\"\"\n",
        "    Stage 2: Assess severity based on damage description\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert Pydantic model to JSON string\n",
        "    claim_info_json = claim_info.model_dump_json()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": severity_assessment_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": claim_info_json},\n",
        "    ]\n",
        "\n",
        "    response = get_completion(messages=messages)\n",
        "\n",
        "    # Gate check: validate the severity assessment\n",
        "    try:\n",
        "        validated_severity = gate2_cost_range_ok(response)\n",
        "        return validated_severity\n",
        "    except ValueError as e:\n",
        "        print(f\"Gate 2 failed: {e}. Response: {response}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SeverityAssessment(severity='Minor', est_cost=200.0),\n",
              " SeverityAssessment(severity='Moderate', est_cost=1500.0),\n",
              " SeverityAssessment(severity='Major', est_cost=15000.0),\n",
              " SeverityAssessment(severity='Moderate', est_cost=3000.0),\n",
              " SeverityAssessment(severity='Minor', est_cost=500.0)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run the claim extraction function on the sample data\n",
        "# No updates needed in this cell\n",
        "\n",
        "severity_assessment_items = [\n",
        "    assess_severity(item) for item in extracted_claim_info_items\n",
        "]\n",
        "\n",
        "severity_assessment_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Stage III: Queue Routing\n",
        "In this stage, we'll route the claim to the appropriate queue based on severity and damage area.\n",
        "\n",
        "Use these routing rules:\n",
        "- 'glass' queue: For Minor damage involving ONLY glass (windshield, windows)\n",
        "- 'fast_track' queue: For other Minor damage\n",
        "- 'material_damage' queue: For all Moderate damage\n",
        "- 'total_loss' queue: For all Major damage\n",
        "\n",
        "These are the priority levels:\n",
        "- Priority 1 (highest): Safety issues, customer stranded\n",
        "- Priority 2: Significant but contained damage, vehicle drivable\n",
        "- Priority 3: Standard claims\n",
        "- Priority 4: Minor issues, no mobility impact\n",
        "- Priority 5 (lowest): Cosmetic only, no functional impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a system prompt for claim routing according to the provided ClaimRouting class\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "class ClaimRouting(BaseModel):\n",
        "    claim_id: str\n",
        "    queue: Literal[\"glass\", \"fast_track\", \"material_damage\", \"total_loss\"]\n",
        "    priority: int = Field(..., ge=1, le=5)\n",
        "\n",
        "\n",
        "queue_routing_system_prompt = \"\"\"\n",
        "You are an auto insurance claim routing specialist. Your task is to determine the appropriate processing queue for each claim.\n",
        "\n",
        "Use the following rules to assign the queue:\n",
        "- glass: If the damage is limited to glass components (e.g., windshield, windows)\n",
        "- fast_track: If the damage is minor (estimated cost <$1,000) and does\n",
        "- material_damage: If the damage is moderate or major but the vehicle is repairable\n",
        "- total_loss: for all major damege\n",
        "\n",
        "Also assign a priority level:\n",
        "- Priority 1: Highest priority, safety issues, customer stranded\n",
        "- Priority 2: Significant but contained damage, vehicle drivable\n",
        "- Priority 3: Standard claims\n",
        "- Priority 4: Minor issues, no mobility impact\n",
        "- Priority 5: Lowest priority, minor cosmetic issues\n",
        "\n",
        "Format your response as a valid JSON object with the following keys:\n",
        "- claim_id (str): The claim ID\n",
        "- queue (str): One of \"glass\", \"fast_track\", \"material_damage\", or \"total_loss\"\n",
        "- priority (int): An integer from 1 (highest) to 5 (lowest)\n",
        "\n",
        "Only respond with the JSON object, nothing else.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a gate check function and assess_severity function\n",
        "# No updates needed in this cell\n",
        "\n",
        "\n",
        "def gate3_validate_routing(routing_json: str) -> ClaimRouting:\n",
        "    \"\"\"\n",
        "    Gate 3: Validates that the claim is routed to a valid queue.\n",
        "    Returns validated ClaimRouting object or raises validation error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON string\n",
        "        routing_dict = json.loads(routing_json)\n",
        "        # Validate with Pydantic model\n",
        "        validated_routing = ClaimRouting(**routing_dict)\n",
        "        return validated_routing\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Gate 3 validation failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def route_claim(\n",
        "    claim_info: ClaimInformation, severity_assessment: Optional[SeverityAssessment]\n",
        ") -> Optional[ClaimRouting]:\n",
        "    \"\"\"\n",
        "    Stage 3: Route claim to appropriate queue\n",
        "    \"\"\"\n",
        "    if severity_assessment is None:\n",
        "        return None\n",
        "\n",
        "    # Create input for the routing model\n",
        "    routing_input = {\n",
        "        \"claim_info\": claim_info.model_dump(),\n",
        "        \"severity_assessment\": severity_assessment.model_dump(),\n",
        "    }\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": queue_routing_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": json.dumps(routing_input)},\n",
        "    ]\n",
        "\n",
        "    response = get_completion(messages=messages)\n",
        "\n",
        "    # Gate check: validate the routing decision\n",
        "    try:\n",
        "        validated_routing = gate3_validate_routing(response)\n",
        "        return validated_routing\n",
        "    except ValueError as e:\n",
        "        print(f\"Gate 3 failed: {e}. Response: {response}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ClaimRouting(claim_id='C001', queue='glass', priority=4),\n",
              " ClaimRouting(claim_id='C002', queue='material_damage', priority=3),\n",
              " ClaimRouting(claim_id='C003', queue='total_loss', priority=1),\n",
              " ClaimRouting(claim_id='C004', queue='material_damage', priority=3),\n",
              " ClaimRouting(claim_id='C005', queue='fast_track', priority=4)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run the routing function on the sample data\n",
        "# No updates needed in this cell\n",
        "\n",
        "routed_claim_items = [\n",
        "    route_claim(claim, severity_assessment)\n",
        "    for claim, severity_assessment in zip(\n",
        "        extracted_claim_info_items, severity_assessment_items\n",
        "    )\n",
        "]\n",
        "\n",
        "routed_claim_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Review Outputs\n",
        "\n",
        "Let's put our data into a pandas dataframe for easier analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# No updates needed in this cell\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m records = []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m claim, severity_assessment, routed_claim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m      7\u001b[39m     extracted_claim_info_items, severity_assessment_items, routed_claim_items\n\u001b[32m      8\u001b[39m ):\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "# No updates needed in this cell\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "records = []\n",
        "for claim, severity_assessment, routed_claim in zip(\n",
        "    extracted_claim_info_items, severity_assessment_items, routed_claim_items\n",
        "):\n",
        "    record = {}\n",
        "    record.update(claim)\n",
        "    record.update(severity_assessment)\n",
        "    record.update(routed_claim)\n",
        "    records.append(record)\n",
        "\n",
        "\n",
        "# Show the entire dataframe since it is not too large\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Reflection & Transfer\n",
        "Reflect on the effectiveness of chaining prompts for this task:\n",
        "\n",
        "1. Prompt Chain Architecture:\n",
        "* How does breaking down the task into stages affect the performance?\n",
        "* What are the benefits of having gate checks between stages?\n",
        "* How could the chain design be improved?\n",
        "\n",
        "2. Error Handling:\n",
        "* What types of errors might occur at each stage?\n",
        "* How could we make the chain more robust against errors?\n",
        "* What would a good fallback strategy look like?\n",
        "\n",
        "3. Scalability:\n",
        "* How well would this approach scale to handle more complex claims?\n",
        "* What challenges might arise when processing thousands of claims?\n",
        "* How could the prompt chain be optimized for efficiency?\n",
        "\n",
        "4. Transfer to Other Domains:\n",
        "* How could this prompt chaining approach be applied to other domains?\n",
        "* What general principles of prompt chaining can we extract from this exercise?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "üéâ Congratulations! üéâ You've built an impressive prompt chain system for insurance claims!\n",
        "You transformed messy FNOL text into structured data, assessed damage severity, and routed claims to the right queues, all with robust gate checks! üöÄ‚ú®\n",
        "\n",
        "Remember:\n",
        "\n",
        "- üîó Chained prompts break complex tasks into manageable steps\n",
        "- üõ°Ô∏è Gate checks prevent error cascades\n",
        "- üß† Having specialized prompts helps keep code focused and maintainable\n",
        "\n",
        "You've mastered a powerful pattern for countless business processes! üèÜ\n",
        "Amazing work on your agentic reasoning system! üíØ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
